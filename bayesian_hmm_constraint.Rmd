---
title: "Bayesian methods for hmm with constraint"
---
### different sigmas 

Generate simulation data
```{r}
set.seed(1234)
# transMtx = matrix(c(0.95,0.015,0.005,0.03,
#                      0.005,0.98,0.010,0.005,
#                      0.015,0.005,0.90,0.08,
#                      0.05,0.01,0.14,0.8),byrow = TRUE,ncol = 4)
# pi = c(0,0.85,0.05,0.1)
# mu = c(-2,-1,1,3)
# sigma = rep(0.9 ,4)

transMtx = matrix(c(0.98,0.015,0.005,
                     0.005,0.98,0.015,
                     0.015,0.005,0.98),byrow = TRUE,ncol = 3)
pi = c(0,0.95,0.05)
mu = c(-2,-1,1)
sigma = rep(0.9,3)

T = 300
M = dim(transMtx)[1]

x = rep(NA,T)
x[1] = sample(1:M,1,prob = pi)
for (i in 2:T){
  x[i] = sample(1:M,1,prob = transMtx[x[i-1],])
}

y = rep(NA,T); y_real = rep(NA,T)
for (i in 1:T){
  idx = x[i]
  y_real[i] = mu[idx]
  y[i] = rnorm(1,y_real[i],sigma[idx])
}

library('ggplot2')
dat = data.frame(id = 1:T, x = x, y = y, y_real = y_real)
p <- ggplot(dat,aes(x = id, y = y_real))+
  geom_line()+
  geom_point(aes(x = id, y = y),size = 0.5)+
  xlim(0,T)+
  ylim(-6,4)
p

NumberofSegments = length(rle(x)$value)
NumberofSegments
```


Forward algorithm with constraint likelihood version

```{r}
library(matrixStats)
lmy_forward_constraint <- function(obs,mu,sigma,transMtx,pi,kmax){
  T = length(obs)
  M = length(pi)
  
  lA = array(NA,dim=c(M,kmax,T))
  transMtx_NoDiag = transMtx - diag(diag(transMtx))
  lpy_x = mapply(obs,FUN = function(x) dnorm(x,mu,sigma,log = TRUE))
  
  ## k = 1 at position 1
  lA[,1,1] =  lpy_x[,1] + log(pi)
  for (s in 2:T){
    ## k = 1
    lA[,1,s] = lA[,1,s-1] + log(diag(transMtx)) + lpy_x[,s]
  }
  if (kmax == 1){ return(lA) }
  
  for (k in 2:kmax){
    for (i in 1:M) { lA[i,k,k] = logSumExp(lA[,k-1,k-1] + log(transMtx_NoDiag[,i])) + lpy_x[i,k] } ## k = k at position k
    if( k < T ){
      for (s in (k+1):T){
        for (i in 1:M){
          tmp = lA[,c(k-1,k),(s-1)] + log(cbind(transMtx_NoDiag[,i],diag(diag(transMtx))[,i]))
          lA[i,k,s] = logSumExp(tmp) + lpy_x[i,s]
        }
      }
    }
  }
  return(lA)
}
```



FFBS with constraint algorithm

```{r}
my_FFBS_constraint = function(obs,mu,sigma,transMtx,pi,kmax){
  T = length(obs)
  M = length(pi)
  success = 0
  
  transMtx_NoDiag = transMtx - diag(diag(transMtx))
  lA = lmy_forward_constraint(obs,mu,sigma,transMtx,pi,kmax)
  llk = logSumExp(lA[,,T])
  
  while(success<1){
    lprob = apply(lA[,,T],2,function(x) logSumExp(x))
    kseg = sample(1:kmax,1,prob = c(exp(lprob-logSumExp(lprob))))
    x = rep(NA,T); s = kseg;
    x[T] = sample(1:M,1,prob = exp(lA[,kseg,T]-logSumExp(lA[,kseg,T])))
    for (j in (T-1):1){
      if(j>=s){
        if(s == 1){
          x[j] = x[j+1]
        }else{
          lprob = lA[,c(s-1,s),j] + log(cbind(transMtx_NoDiag[,x[j+1]],diag(diag(transMtx))[,x[j+1]]))
          tmp = sample(1:length(lprob),1,prob = c(exp(lprob-logSumExp(lprob))))
          x[j] = ifelse(tmp <= M,tmp,tmp-M)
          s = s - (x[j] != x[j+1])
        }
      #print(paste(s,j,x[j],sep = ";"))
      }
    }
    if(!is.na(x[1])){
      success = success+1
    }
  }
  return(list("x"=x,"llk"=llk))
}
```


FB Gibbs sampler with constraint
by EM versus Markov chain Monte Carlo for Estimation of Hidden Markov Models: A Computational Perspective(2008)
1
```{r}
library("gtools")
library("truncnorm")
FB_sampler_one <- function(y,mu,sigma2_inv,beta,A,pi,x,K,epsilon,kappa,alpha,g,h,kmax){
  sigma2 = 1/sigma2_inv
  ##### update mu
  for(i in 1:K){
    S = sum(y[x==i])
    n = sum(x==i)
    ## no constraint
    #mu[i] = rnorm(1,(S+kappa*epsilon*sigma2)/(n+kappa*sigma2),sqrt((sigma2)/(n+kappa*sigma2)))
    ## truncated normal
    mu[i] = rtruncnorm(n = 1,a = max(-Inf,mu[i-1],na.rm = TRUE),b = min(Inf,mu[i+1],na.rm = TRUE),mean = (S+kappa*epsilon*sigma2[i])/(n+kappa*sigma2[i]),sd = sqrt((sigma2[i])/(n+kappa*sigma2[i])))
  }
  
  ##### update sigma2
  for(i in 1:K){
    n = sum(x==i)
    Sq = sum((y[x==i]-mu[i])^2)
    sigma2_inv[i] = rgamma(1,alpha+0.5*n,beta+0.5*Sq)
  }
  sigma2 = 1/sigma2_inv
  
  ##### update beta
  beta = rgamma(1,g+3*alpha,h+sum(sigma2_inv))
  
  ##### update A
  N_trans = matrix(0,ncol=K,nrow = K)
  for(i in 1:T-1)
    N_trans[x[i],x[i+1]] = N_trans[x[i],x[i+1]] + 1
  for(i in 1:K){
    A[i,] = rdirichlet(1,N_trans[i,]+1)
  }
  
  ##### update pi
  pi = rdirichlet(1,rep(1,K)+(x[1] == 1:K))
  
  ##### update x
  tmp = my_FFBS_constraint(y,mu,sqrt(sigma2),A,pi,kmax)
  x = tmp$x; llk = tmp$llk  ##### calculate after x was updated?
  
  return(list("pi"=pi,"A"=A,"mu"=mu,"beta"=beta,"sigma2_inv"=sigma2_inv,"x"=x,"llk"=llk))
}
```



```{r}
FB_sampler_constraint <- function(data, K, kmax, burnin = 200, thin = 1, num_sample = 1000){
  ## some constants from data
  y = data;
  T = length(y)
  epsilon = sum(range(y))/2
  R = diff(range(y))
  kappa = 1/R^2
  num = 1
  
  ## hyperparameters constant
  alpha = 2
  g = 0.2
  h = 10 / R^2
  
  ## random start point
  pi = rep(1/K,K)
  A = matrix(rep(1/K,K*K),ncol = K)
  mu = sort(rnorm(K,epsilon,1/kappa))
  beta = rgamma(1,g,h)
  sigma2_inv = rep(rgamma(1,alpha,beta),K)
  x = sample(1:K,T,replace = TRUE)
  
  ## gibbs sampler
  ### initialize storage
  pi_sam = matrix(0,nrow = num_sample,ncol = K)
  A_sam = array(0,dim=c(K,K,num_sample))
  mu_sam = matrix(0,nrow = num_sample,ncol = K)
  beta_sam = rep(0,num_sample)
  sigma2_inv_sam = matrix(0,nrow = num_sample,ncol = K)
  x_sam = matrix(0,nrow = num_sample,ncol = T)
  llk_sam = rep(0,num_sample)
  
  ### sampler start
  #### burnin step
  for(iter in 1: burnin){
    hmm_sample = FB_sampler_one(y,mu,sigma2_inv,beta,A,pi,x,K,epsilon,kappa,alpha,g,h,kmax)
    mu = hmm_sample$mu
    sigma2_inv = hmm_sample$sigma2_inv
    beta = hmm_sample$beta
    A = hmm_sample$A
    pi = hmm_sample$pi
    x = hmm_sample$x
  }
  
  ##### sampling
  for (iter in 1:(thin*num_sample)){
    hmm_sample = FB_sampler_one(y,mu,sigma2_inv,beta,A,pi,x,K,epsilon,kappa,alpha,g,h,kmax)
    mu = hmm_sample$mu
    sigma2_inv = hmm_sample$sigma2_inv
    beta = hmm_sample$beta
    A = hmm_sample$A
    pi = hmm_sample$pi
    x = hmm_sample$x
    llk = hmm_sample$llk
  
    ##### store the samples
    if(iter%%thin==0){
      pi_sam[num,] = pi
      A_sam[,,num] = A
      mu_sam[num,] = mu
      beta_sam[num] = beta
      sigma2_inv_sam[num,] = sigma2_inv
      x_sam[num,] = x
      llk_sam[num] = llk
      num = num + 1
    }
  }
  
  return(list("pi"=pi_sam,"A"=A_sam,"mu"=mu_sam,"beta"=beta_sam,"sd"=sqrt(1/sigma2_inv_sam),"x"=x_sam,"llk"=llk_sam))
}
```

```{r}
#result = FB_sampler_constraint(data = y,K = 3,kmax = 15,burnin = 100,thin = 1,num_sample = 200)
#plot(apply(result$x,2,mean),type="l")
#plot(x,type="l")
dat = data.frame(id = 1:T, x = x, x_posmean = apply(result$x,2,mean))
library("reshape2")
datm = melt(dat,id.vars = 1)
p <- ggplot(datm,aes(x = id, y = value))+
  geom_line(aes( color = variable,group = variable))+
  scale_color_manual(values = c("red","green"),labels = c("x_real","x_viterbi"))+
  xlim(0,T)+
  ylim(0,5)
p
apply(result$mu,-1,mean)
apply(result$sd,-1,mean)
length(rle(apply(result$x,2,mean))$values)
```

```{r}
s = apply(result$x,1,function(x) length(rle(x)$values))
hist(s)
```


```{r}
## kmax: max num of segments
## smax: max num of states

Determine_num_state_constraint = function(y,smax = 5,kmax = NumberofSegments+10,burnin=1000,thin=1,num_sample=1000){
  lprobS = matrix(0,ncol = smax,nrow = num_sample)
  
  ## calculate likelihood eq21
  for(k in 2:smax){ ## k can not be 1
    lprobS[,k] = FB_sampler_constraint(data = y,K = k,kmax = kmax, burnin = burnin,num_sample = num_sample)$llk
  }
  
  ## add prior
  lprobS = lprobS + log(1/Kmax)
  
  ## normalization
  lprobS = apply(lprobS,1,function(x) x-logSumExp(x))
  
  ## mc calculation eq21
  postS = apply(lprobS,1, function(x) exp(logSumExp(x))/num_sample)
  return(postS)
  
}
```

```{r}
smax = 6
postS = Determine_num_state_constraint(y = y,smax = smax,kmax = NumberofSegments+10,burnin = 1000,num_sample = 1000)
postS
p <- ggplot(data.frame(id=1:Kmax,prob=postS),aes(x=id,y=prob)) + 
  geom_bar(stat = "identity",position = "dodge",width = 0.6)
p
```

