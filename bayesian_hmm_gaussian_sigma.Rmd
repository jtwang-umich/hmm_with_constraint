---
title: "Bayesian methods for hmm"
---
### different sigmas 

Generate simulation data
```{r}
set.seed(1234)
# transMtx = matrix(c(0.95,0.015,0.005,0.03,
#                      0.005,0.98,0.010,0.005,
#                      0.015,0.005,0.90,0.08,
#                      0.05,0.01,0.14,0.8),byrow = TRUE,ncol = 4)
# pi = c(0,0.85,0.05,0.1)
# mu = c(-2,-1,1,3)
# sigma = rep(0.9 ,4)

transMtx = matrix(c(0.98,0.015,0.005,
                     0.005,0.98,0.015,
                     0.015,0.005,0.98),byrow = TRUE,ncol = 3)
pi = c(0,0.95,0.05)
mu = c(-2,-1,1)
sigma = c(0.1,0.2,0.8)

T = 300
M = dim(transMtx)[1]

x = rep(NA,T)
x[1] = sample(1:M,1,prob = pi)
for (i in 2:T){
  x[i] = sample(1:M,1,prob = transMtx[x[i-1],])
}

y = rep(NA,T); y_real = rep(NA,T)
for (i in 1:T){
  idx = x[i]
  y_real[i] = mu[idx]
  y[i] = rnorm(1,y_real[i],sigma[idx])
}

library('ggplot2')
dat = data.frame(id = 1:T, x = x, y = y, y_real = y_real)
p <- ggplot(dat,aes(x = id, y = y_real))+
  geom_line()+
  geom_point(aes(x = id, y = y),size = 0.5)+
  xlim(0,T)+
  ylim(-6,4)
p

NumberofSegments = length(rle(x)$value)
NumberofSegments
```


Forward algorithm likelihood version
1
```{r}
library(matrixStats)
lmy_forward_gaussian <- function(obs,mu,sigma,transMtx,pi){
  T = length(obs)
  M = length(pi)
  
  lA = array(NA,dim=c(M,T))
  lA[,1] =  dnorm(obs[1],mu,sigma,log = TRUE) + log(pi)
  
  for (j in 2:T){
    lpy_x = dnorm(obs[j],mu,sigma,log = TRUE)
    for (i in 1:M){
      lA[i,j] = logSumExp(lA[,j-1] + log(transMtx[,i]) + lpy_x[i])
    }
  }
  
  return(lA)
}
```



FFBS algorithm
1
```{r}
my_FFBS = function(obs,mu,sigma,transMtx,pi){
  T = length(obs)
  M = length(pi)
  
  lA = lmy_forward_gaussian(obs,mu,sigma,transMtx,pi)
  
  x = rep(NA,T);
  x[T] = sample(1:M,1,prob = exp(lA[,T]-logSumExp(lA[,T])))

  for (j in (T-1):1){
    lprob = lA[,j] + log(transMtx[,x[j+1]])
    x[j] = sample(1:M,1,prob = c(exp(lprob-logSumExp(lprob))))
  }
  llk = logSumExp(lA[,T])
  
  return(list("x"=x,"llk"=llk))
}
```


FB Gibbs sampler
by EM versus Markov chain Monte Carlo for Estimation of Hidden Markov Models: A Computational Perspective(2008)
1
```{r}
library("gtools")
library("truncnorm")
FB_sampler_one <- function(y,mu,sigma2_inv,beta,A,pi,x,K,epsilon,kappa,alpha,g,h){
  sigma2 = 1/sigma2_inv
  ##### update mu
  for(i in 1:K){
    S = sum(y[x==i])
    n = sum(x==i)
    ## no constraint
    #mu[i] = rnorm(1,(S+kappa*epsilon*sigma2)/(n+kappa*sigma2),sqrt((sigma2)/(n+kappa*sigma2)))
    ## truncated normal
    mu[i] = rtruncnorm(n = 1,a = max(-Inf,mu[i-1],na.rm = TRUE),b = min(Inf,mu[i+1],na.rm = TRUE),mean = (S+kappa*epsilon*sigma2[i])/(n+kappa*sigma2[i]),sd = sqrt((sigma2[i])/(n+kappa*sigma2[i])))
  }
  
  ##### update sigma2
  for(i in 1:K){
    n = sum(x==i)
    Sq = sum((y[x==i]-mu[i])^2)
    sigma2_inv[i] = rgamma(1,alpha+0.5*n,beta+0.5*Sq)
  }
  sigma2 = 1/sigma2_inv
  
  ##### update beta
  beta = rgamma(1,g+3*alpha,h+sum(sigma2_inv))
  
  ##### update A
  N_trans = matrix(0,ncol=K,nrow = K)
  for(i in 1:T-1)
    N_trans[x[i],x[i+1]] = N_trans[x[i],x[i+1]] + 1
  for(i in 1:K){
    A[i,] = rdirichlet(1,N_trans[i,]+1)
  }
  
  ##### update pi
  pi = rdirichlet(1,rep(1,K)+(x[1] == 1:K))
  
  ##### update x
  tmp = my_FFBS(y,mu,sqrt(sigma2),A,pi)
  x = tmp$x; llk = tmp$llk  ##### calculate after x was updated?
  
  return(list("pi"=pi,"A"=A,"mu"=mu,"beta"=beta,"sigma2_inv"=sigma2_inv,"x"=x,"llk"=llk))
}
```



```{r}
FB_sampler <- function(data, K, burnin = 200, thin = 1, num_sample = 1000){
  ## some constants from data
  y = data;
  T = length(y)
  epsilon = sum(range(y))/2
  R = diff(range(y))
  kappa = 1/R^2
  num = 1
  
  ## hyperparameters constant
  alpha = 2
  g = 0.2
  h = 10 / R^2
  
  ## random start point
  pi = rep(1/K,K)
  A = matrix(rep(1/K,K*K),ncol = K)
  mu = sort(rnorm(K,epsilon,1/kappa))
  beta = rgamma(1,g,h)
  sigma2_inv = rep(rgamma(1,alpha,beta),K)
  x = sample(1:K,T,replace = TRUE)
  
  ## gibbs sampler
  ### initialize storage
  pi_sam = matrix(0,nrow = num_sample,ncol = K)
  A_sam = array(0,dim=c(K,K,num_sample))
  mu_sam = matrix(0,nrow = num_sample,ncol = K)
  beta_sam = rep(0,num_sample)
  sigma2_inv_sam = matrix(0,nrow = num_sample,ncol = K)
  x_sam = matrix(0,nrow = num_sample,ncol = T)
  llk_sam = rep(0,num_sample)
  
  ### sampler start
  #### burnin step
  for(iter in 1: burnin){
    hmm_sample = FB_sampler_one(y,mu,sigma2_inv,beta,A,pi,x,K,epsilon,kappa,alpha,g,h)
    mu = hmm_sample$mu
    sigma2_inv = hmm_sample$sigma2_inv
    beta = hmm_sample$beta
    A = hmm_sample$A
    pi = hmm_sample$pi
    x = hmm_sample$x
  }
  
  ##### sampling
  for (iter in 1:(thin*num_sample)){
    hmm_sample = FB_sampler_one(y,mu,sigma2_inv,beta,A,pi,x,K,epsilon,kappa,alpha,g,h)
    mu = hmm_sample$mu
    sigma2_inv = hmm_sample$sigma2_inv
    beta = hmm_sample$beta
    A = hmm_sample$A
    pi = hmm_sample$pi
    x = hmm_sample$x
    llk = hmm_sample$llk
  
    ##### store the samples
    if(iter%%thin==0){
      pi_sam[num,] = pi
      A_sam[,,num] = A
      mu_sam[num,] = mu
      beta_sam[num] = beta
      sigma2_inv_sam[num,] = sigma2_inv
      x_sam[num,] = x
      llk_sam[num] = llk
      num = num + 1
    }
  }
  
  return(list("pi"=pi_sam,"A"=A_sam,"mu"=mu_sam,"beta"=beta_sam,"sd"=sqrt(1/sigma2_inv_sam),"x"=x_sam,"llk"=llk_sam))
}
```

```{r}
result = FB_sampler(y,3,500,1,500)
#plot(apply(result$x,2,mean),type="l")
#plot(x,type="l")
dat = data.frame(id = 1:T, x = x, x_posmean = apply(result$x,2,mean))
library("reshape2")
datm = melt(dat,id.vars = 1)
p <- ggplot(datm,aes(x = id, y = value))+
  geom_line(aes( color = variable,group = variable))+
  scale_color_manual(values = c("red","green"),labels = c("x_real","x_viterbi"))+
  xlim(0,T)+
  ylim(0,5)
p
apply(result$mu,-1,mean)
apply(result$sd,-1,mean)
length(rle(apply(result$x,2,mean))$values)
```

```{r}
Determine_num_state = function(y,Kmax = 5,burnin=200,thin=1,num_sample=1000){
  lprobS = matrix(0,ncol = Kmax,nrow = num_sample)
  
  ## calculate likelihood eq21
  for(k in 1:Kmax){
    lprobS[,k] = FB_sampler(data = y,K = k,burnin = burnin,num_sample = num_sample)$llk
  }
  
  ## add prior
  lprobS = lprobS + log(1/Kmax)
  
  ## normalization
  lprobS = apply(lprobS,1,function(x) x-logSumExp(x))
  
  ## mc calculation eq21
  postS = apply(lprobS,1, function(x) exp(logSumExp(x))/num_sample)
  return(postS)
  
}
```

```{r}
Kmax = 6
postS = Determine_num_state(y,Kmax,burnin = 500,num_sample = 500)
postS
p <- ggplot(data.frame(id=1:Kmax,prob=postS),aes(x=id,y=prob)) + 
  geom_bar(stat = "identity",position = "dodge",width = 0.6)
p
```

